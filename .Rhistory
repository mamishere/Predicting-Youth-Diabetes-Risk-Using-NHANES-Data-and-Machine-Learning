library(e1071)
library(klaR)
#######################################
### Second Step: Data Preprocessing ###
#######################################
# Read the Earning.csv file into a data frame.
df <- read.csv("./Datasets/final_dataset.csv")
# Print the dimensions of the dataset.
cat("The final dataset's shape:", dim(df), "\n")
# Display the first few rows of the dataset.
head(df, 5)
# Function to calculate the percentage of non-missing values.
percentage_non_missing <- function(x) {
mean(!is.na(x)) * 100
}
# Apply the function to each column in the dataframe.
non_missing_percentages <- sapply(df, percentage_non_missing)
# Create a new dataframe to store the results without row names.
result_df <- data.frame(
Feature = names(df),
Non_Missing_Percentage = non_missing_percentages,
row.names = NULL
)
# Print the results.
result_df
#############################################
### Third Step: Exploratory Data Analysis ###
#############################################
# Extract column names from the dataframe.
features <- names(df)
# Display the extracted column names.
features
# Define categorical features.
cat_features <- c("Gender", "Race", "Diabetes.Risk", "Hypertension", "ADA.Biomarker.Label", "ADA.Screening.Label")
# Identify numeric features, excluding specific columns.
num_features <- setdiff(features, cat_features)
num_features <- num_features[num_features != "ID"]
# Display categorical features.
cat("Categorical features:")
cat("\n")
cat(cat_features)
cat("\n\n")
# Display numeric features.
cat("Numeric features:")
cat("\n")
cat(num_features)
# Function to create a summary table for categorical features.
cat_summary_table <- function(dataframe, categorical_features) {
# Create an empty data frame to store the results
summary_table <- data.frame(
Feature = character(),
Unique_Values = character(),
Percentage = numeric(),
stringsAsFactors = FALSE
)
# Loop through each specified feature
for (feature in categorical_features) {
# Extract unique values and their counts for the current feature
value_counts <- table(dataframe[[feature]])
# Calculate percentages
percentages <- round(prop.table(value_counts) * 100, 0)
# Append information to the table data frame
summary_table <- rbind(summary_table, data.frame(
Feature = rep(feature, length(value_counts)),
Unique_Values = as.character(names(value_counts)),
Percentage = as.numeric(percentages)
))
}
# Remove duplicate feature names, leaving only one occurrence
feature_feature <- c(summary_table$Feature)
feature_feature[duplicated(feature_feature)] <- ""
summary_table$Feature <- feature_feature
# Return the resulting table
return(summary_table)
}
# Generate and display the summary table
summary_table <- cat_summary_table(df, cat_features)
summary_table
# Function to create a summary table for numeric features.
num_summary_table <- function(dataframe, numeric_features) {
# Extract the numeric columns from the dataframe
numeric_data <- dataframe[numeric_features]
# Compute summary statistics
summary_stats <- data.frame(
Feature = character(),
Min = numeric(),
Q1 = numeric(),
Mean = numeric(),
Median = numeric(),
Q3 = numeric(),
Max = numeric(),
stringsAsFactors = FALSE
)
for (feature in numeric_features) {
summary_stats <- rbind(
summary_stats,
c(
Feature = feature,
Min = round(min(numeric_data[[feature]], na.rm = TRUE), 2),
Q1 = round(quantile(numeric_data[[feature]], 0.25, na.rm = TRUE), 2),
Mean = round(mean(numeric_data[[feature]], na.rm = TRUE), 2),
Median = round(median(numeric_data[[feature]], na.rm = TRUE), 2),
Q3 = round(quantile(numeric_data[[feature]], 0.75, na.rm = TRUE), 2),
Max = round(max(numeric_data[[feature]], na.rm = TRUE), 2)
)
)
}
# Set the column names for the summary dataframe
colnames(summary_stats) <- c("Feature", "Min", "Q1", "Mean", "Median", "Q3", "Max")
return(summary_stats)
}
# Generate and display the summary table for numeric features
summary_table <- num_summary_table(df, num_features)
summary_table
df <- df[df$Diastolic.BP >= 4,]
# Function to create a summary table for numeric features.
num_summary_table <- function(dataframe, numeric_features) {
# Extract the numeric columns from the dataframe
numeric_data <- dataframe[numeric_features]
# Compute summary statistics
summary_stats <- data.frame(
Feature = character(),
Min = numeric(),
Q1 = numeric(),
Mean = numeric(),
Median = numeric(),
Q3 = numeric(),
Max = numeric(),
stringsAsFactors = FALSE
)
for (feature in numeric_features) {
summary_stats <- rbind(
summary_stats,
c(
Feature = feature,
Min = round(min(numeric_data[[feature]], na.rm = TRUE), 2),
Q1 = round(quantile(numeric_data[[feature]], 0.25, na.rm = TRUE), 2),
Mean = round(mean(numeric_data[[feature]], na.rm = TRUE), 2),
Median = round(median(numeric_data[[feature]], na.rm = TRUE), 2),
Q3 = round(quantile(numeric_data[[feature]], 0.75, na.rm = TRUE), 2),
Max = round(max(numeric_data[[feature]], na.rm = TRUE), 2)
)
)
}
# Set the column names for the summary dataframe
colnames(summary_stats) <- c("Feature", "Min", "Q1", "Mean", "Median", "Q3", "Max")
return(summary_stats)
}
# Generate and display the summary table for numeric features
summary_table <- num_summary_table(df, num_features)
summary_table
# Load necessary libraries
library(ggplot2)
# Create a new column 'Color' based on BMI.Percentile
df$Color <- cut(df$BMI.Percentile, breaks = c(-Inf, 75, 85, Inf),
labels = c("green", "orange", "red"))
# Scatter plot with normalized values and limited axis range
ggplot(df, aes(x = scale(X2hrPG), y = scale(FPG), color = Color, size = HbA1c)) +
geom_point(alpha = 0.4) +
scale_color_manual(values = c("green", "orange", "red")) +
scale_size_continuous(range = c(1, 10)) +
labs(title = "Scatter Plot of Normalized HbA1c vs Normalized FPG",
x = "Normalized 2hrPG",
y = "Normalized FPG") +
coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3)) +
# Increase text size
theme(
text = element_text(size = 16),  # You can adjust the size as needed
axis.title = element_text(size = 18),
axis.text = element_text(size = 14),
legend.text = element_text(size = 14),
legend.title = element_text(size = 16),
plot.title = element_text(size = 20)
)
# Remove the 'Color' column from the dataframe
df <- subset(df, select = -Color)
# Create a scatter plot using ggplot with color based on ADA Biomarker and less jitter
ggplot(df, aes(x = Age, y = Total.Cholestrol, color = factor(ADA.Biomarker.Label))) +
geom_jitter(alpha = 0.6, width = 0.1) +  # Adjust width for less jitter
scale_color_manual(values = c("0" = "grey", "1" = "red")) +  # Set colors for ADA Biomarker values
labs(title = "Scatter Plot of Age vs Total Cholestrol",
x = "Age",
y = "Total Cholestrol",
color = "ADA Biomarker") +  # Legend title for color scale
scale_x_continuous(breaks = unique(df$Age)) +  # Show all ages on the x-axis
theme(
text = element_text(size = 14),  # Set the base text size
axis.text.x = element_text(size = 12),  # Set x-axis text size
axis.text.y = element_text(size = 12),  # Set y-axis text size
axis.title.x = element_text(size = 14),  # Set x-axis label size
axis.title.y = element_text(size = 14),  # Set y-axis label size
plot.title = element_text(size = 16),  # Set plot title size
legend.title = element_text(size = 14),  # Set legend title size
legend.text = element_text(size = 12)  # Set legend text size
)
######################################################
### Fourth and Final Step: Modeling and Examination ###
######################################################
# Convert specified categorical features to factor type
for (feature in cat_features) {
df[[feature]] <- factor(df[[feature]])
}
# Create a data frame with column names and data types
dtypes_df <- data.frame(
Feature = names(df),
Data_Type = sapply(df, class),
row.names = NULL
)
# Display the data frame
dtypes_df
# Set a seed for reproducibility.
set.seed(123)
# Split the dataset into training and testing sets based on the ADA Biomarker Label.
split <- sample.split(df$ADA.Biomarker.Label, SplitRatio = 0.8)
# Create the train set by subsetting rows where the split is TRUE.
train_set <- subset(df, split == TRUE)
# Create the test set by subsetting rows where the split is FALSE.
test_set <- subset(df, split == FALSE)
# Display the dimensions (number of rows and columns) of the train and test sets.
cat("Train set shape equals to:", dim(train_set), "\n")
cat("Test set shape equals to: ", dim(test_set))
# Select specific variables from the dataframe
variables <- c("Race", "Total.Cholestrol", "Diabetes.Risk",
"BMI.Percentile", "Hypertension", "ADA.Biomarker.Label")
# Display the first five rows of the selected variables
head(df[, variables], 5)
# Calculate the threshold based on the proportion of positive ADA Biomarker labels in the training set
threshold <- round((dim(subset(train_set, ADA.Biomarker.Label == 1))[1] / dim(train_set)[1]), 2)
# Display the calculated threshold
threshold
# Train a logistic regression model using the specified features
lr_model <- logistic_reg(
mixture=double(1),
penalty=double(1),
) %>%
set_engine("glmnet") %>%
set_mode("classification") %>%
fit(
ADA.Biomarker.Label ~ .,
data=train_set[,variables])
# Display model summary
tidy(lr_model)
# Generate class predictions on the training set
train_predictions <- predict(lr_model,
new_data = train_set[, variables],
type = "prob")
# Display the first five rows of the predicted probabilities
head(train_predictions, 5)
# Convert predicted probabilities to binary class predictions ("Yes" or "No").
train_predictions <- ifelse(train_predictions[, 2] > threshold, 1, 0)
# Add the predicted classes to the training set as a new column "LogReg_Pred".
train_set["LogReg_Pred"] <- train_predictions
# Display the first five rows of the updated training set
head(train_set, 5)
# Create a confusion matrix using the predicted and actual labels in the training set.
conf_matrix <- confusionMatrix(
factor(train_set$LogReg_Pred),
train_set$ADA.Biomarker.Label,
positive = "1"
)
# Display the confusion matrix.
conf_matrix
# Generate class predictions on the test set
test_predictions <- predict(lr_model,
new_data = test_set[, variables],
type = "prob")
# Display the first five rows of the predicted probabilities
head(test_predictions, 5)
# Convert predicted probabilities to binary class predictions ("Yes" or "No").
test_predictions <- ifelse(test_predictions[, 2] > threshold, 1, 0)
# Add the predicted classes to the test set as a new column "LogReg_Pred".
test_set["LogReg_Pred"] <- test_predictions
# Display the first five rows of the updated test set
head(test_set, 5)
# Create a confusion matrix using the predicted and actual labels in the test set.
conf_matrix <- confusionMatrix(
factor(test_set$LogReg_Pred),
test_set$ADA.Biomarker.Label,
positive = "1"
)
# Display the confusion matrix.
conf_matrix
# Fit a decision tree model using the rpart function from caret.
caret_model <- rpart(
ADA.Biomarker.Label ~ .,
data = train_set[, variables],
minbucket = 4
)
# Display a summary of the decision tree model.
summary(caret_model)
# Set the background color to white.
par(bg = "white", mar = c(1, 1, 1, 1))
# Plot the decision tree using the prp function.
prp(caret_model,                 # The DT model.
type = 0,                    # Type 0: draw each node as a box.
extra = 0,                   # Draw branch labels.
varlen = 0,                  # Control the width of variable labels.
faclen = 0,                  # Control the width of factor level labels.
branch.lty = 1,              # Set line type for branches.
branch.lwd = 2,              # Set line width for branches.
tweak = 0.7,                 # Adjust the position of node labels.
box.col = "lightblue",       # Set box color.
col = "darkgreen",           # Set text color.
split.col = "red",           # Set color for split points.
fallen.leaves = TRUE         # Draw leaves in line with branch.
)
# Make predictions on the training set using the decision tree model.
train_predictions <- predict(
caret_model,
newdata = train_set[, variables],
type = "prob"
)
# Convert predicted probabilities to binary class predictions ("Yes" or "No").
train_predictions <- ifelse(train_predictions[, 2] > threshold, 1, 0)
# Add the predicted classes to the training set as a new column "CARET_Pred".
train_set["CARET_Pred"] <- train_predictions
# Display the first five rows of the updated training set.
head(train_set, 5)
# Create a confusion matrix using the predicted and actual labels in the training set.
conf_matrix <- confusionMatrix(
factor(train_set$CARET_Pred),
train_set$ADA.Biomarker.Label,
positive = "1"
)
# Display the confusion matrix.
conf_matrix
# Make predictions on the test set using the decision tree model.
test_predictions <- predict(
caret_model,
newdata = test_set[, variables],
type = "prob"
)
# Convert predicted probabilities to binary class predictions ("Yes" or "No").
test_predictions <- ifelse(test_predictions[, 2] > threshold, 1, 0)
# Add the predicted classes to the test set as a new column "CARET_Pred".
test_set["CARET_Pred"] <- test_predictions
# Display the first five rows of the updated test set.
head(test_set, 5)
# Create a confusion matrix using the predicted and actual labels in the test set.
conf_matrix <- confusionMatrix(
factor(test_set$CARET_Pred),
test_set$ADA.Biomarker.Label,
positive = "1"
)
# Display the confusion matrix.
conf_matrix
# Train a random forest model using the randomForest function.
rf_model <- randomForest(
ADA.Biomarker.Label ~ .,
data = train_set[, variables],
ntree = 1000,
importance = TRUE,
proximity = TRUE
)
# Display a summary of the random forest model.
summary(rf_model)
# Set the background color to white.
par(bg = "white", mar = c(1, 1, 1, 1))
# Plotting the random forest model.
plot(rf_model)
# Create an importance plot for the random forest model.
importance(rf_model)
# Set the background color to white.
par(bg = "white", mar = c(1, 1, 1, 1))
# Create a variable importance plot for the random forest model.
varImpPlot(rf_model, , cex = 0.8)
# Make predictions on the training set using the random forest model.
train_predictions <- predict(
rf_model,
newdata = train_set[, variables],
type = "prob"
)
# Convert predicted probabilities to binary class predictions ("Yes" or "No").
train_predictions <- ifelse(train_predictions[, 2] > threshold, 1, 0)
# Add the predicted classes to the training set as a new column "RF_Pred".
train_set["RF_Pred"] <- train_predictions
# Display the first five rows of the updated training set.
head(train_set, 5)
# Create a confusion matrix using the predicted and actual labels in the training set.
conf_matrix <- confusionMatrix(
factor(train_set$RF_Pred),
train_set$ADA.Biomarker.Label,
positive = "1"
)
# Display the confusion matrix.
conf_matrix
# Make predictions on the test set using the random forest model.
test_predictions <- predict(
rf_model,
newdata = test_set[, variables],
type = "prob"
)
# Convert predicted probabilities to binary class predictions ("Yes" or "No").
test_predictions <- ifelse(test_predictions[, 2] > threshold, 1, 0)
# Add the predicted classes to the test set as a new column "RF_Pred".
test_set["RF_Pred"] <- test_predictions
# Display the first five rows of the updated test set.
head(test_set, 5)
# Create a confusion matrix using the predicted and actual labels in the test set.
conf_matrix <- confusionMatrix(
factor(test_set$RF_Pred),
test_set$ADA.Biomarker.Label,
positive = "1"
)
# Display the confusion matrix.
conf_matrix
###########################################
### Supplementary: Improving the Models ###
###########################################
final_df <- df[, variables]
head(final_df, 5)
table(final_df$ADA.Biomarker.Label)
oversampled_data <- ovun.sample(ADA.Biomarker.Label ~ ., data = final_df, method = "over", N = 3832, seed = 123)
df_oversampled <- data.frame(oversampled_data$data)
table(df_oversampled$ADA.Biomarker.Label)
set.seed(123)
split <- sample.split(final_df$ADA.Biomarker.Label, SplitRatio = 0.8)
train_set <- subset(final_df, split == TRUE)
test_set <- subset(final_df, split == FALSE)
cat("Train set shape equals to:", dim(train_set), "\n")
cat("Test set shape equals to: ", dim(test_set))
formula <- ADA.Biomarker.Label ~ Race + Total.Cholestrol + Diabetes.Risk + BMI.Percentile + Hypertension
log_reg_model <- glm(formula, data = train_set, family = "binomial")
control <- trainControl(method = "cv", number = 5)
cv_results <- train(formula, data = train_set, method = "glm", family = "binomial", trControl = control)
cv_results
train_pred <- predict(cv_results, newdata = train_set, type = "prob")
threshold <- 0.3
train_predictions <- ifelse(train_pred[, 2] > threshold, 1, 0)
conf_matrix <- confusionMatrix(
factor(train_predictions),
train_set$ADA.Biomarker.Label,
positive = "1"
)
conf_matrix
test_pred <- predict(cv_results, newdata = test_set, type = "prob")
threshold <- 0.3
test_predictions <- ifelse(test_pred[, 2] > threshold, 1, 0)
conf_matrix <- confusionMatrix(
factor(test_predictions),
test_set$ADA.Biomarker.Label,
positive = "1"
)
conf_matrix
formula <- ADA.Biomarker.Label ~ Race + Total.Cholestrol + Diabetes.Risk + BMI.Percentile + Hypertension
num_fold <- trainControl(method = "cv", number = 10)
cp_grid <- expand.grid(.cp=seq(0.001, 0.2, 0.001))
train(formula, data = train_set, method = "rpart", trControl = num_fold, tuneGrid = cp_grid)
cv_results <- rpart(formula, data = train_set, method = "class", cp = 0.007)
cv_results
train_pred <- predict(cv_results, newdata = train_set, type = "prob")
threshold <- 0.3
train_predictions <- ifelse(train_pred[, 2] > threshold, 1, 0)
conf_matrix <- confusionMatrix(
factor(train_predictions),
train_set$ADA.Biomarker.Label,
positive = "1"
)
conf_matrix
test_pred <- predict(cv_results, newdata = test_set, type = "prob")
threshold <- 0.3
test_predictions <- ifelse(test_pred[, 2] > threshold, 1, 0)
conf_matrix <- confusionMatrix(
factor(test_predictions),
test_set$ADA.Biomarker.Label,
positive = "1"
)
conf_matrix
formula <- ADA.Biomarker.Label ~ Race + Total.Cholestrol + Diabetes.Risk + BMI.Percentile + Hypertension
num_fold <- trainControl(method = "cv", number = 10)
rf_model <- train(formula, data = train_set, method = "rf", trControl = num_fold)
rf_model
train_pred <- predict(rf_model, newdata = train_set, type = "prob")
threshold <- 0.2
train_predictions <- ifelse(train_pred[, 2] > threshold, 1, 0)
conf_matrix <- confusionMatrix(
factor(train_predictions),
train_set$ADA.Biomarker.Label,
positive = "1"
)
conf_matrix
test_pred <- predict(rf_model, newdata = test_set, type = "prob")
threshold <- 0.2
test_predictions <- ifelse(test_pred[, 2] > threshold, 1, 0)
conf_matrix <- confusionMatrix(
factor(test_predictions),
test_set$ADA.Biomarker.Label,
positive = "1"
)
conf_matrix
formula <- ADA.Biomarker.Label ~ Race + Total.Cholestrol + Diabetes.Risk + BMI.Percentile + Hypertension
num_fold <- trainControl(method = "cv", number = 10)
nb_model <- naiveBayes(formula, data = train_set, laplace = 1)
nb_model
train_pred <- predict(nb_model, newdata = train_set, type = "raw")
threshold <- 0.31
train_predictions <- ifelse(train_pred[, 2] > threshold, 1, 0)
conf_matrix <- confusionMatrix(
factor(train_predictions),
train_set$ADA.Biomarker.Label,
positive = "1"
)
conf_matrix
test_pred <- predict(nb_model, newdata = test_set, type = "raw")
threshold <- 0.31
test_predictions <- ifelse(test_pred[, 2] > threshold, 1, 0)
conf_matrix <- confusionMatrix(
factor(test_predictions),
test_set$ADA.Biomarker.Label,
positive = "1"
)
conf_matrix
